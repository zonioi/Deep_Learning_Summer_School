{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfjiNhDED3aU"
      },
      "source": [
        "# Minimal NN to regress four-vector masses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHmaA1MBEOTD"
      },
      "source": [
        "This notebook is intended to find the minimal network structure that is needed to regress the mass of particle four-vectors, given either their\n",
        "- energy and momentum compotents `(e, px, py, pz)`, or\n",
        "- energy, transverse momentum as well as pseudo-rapidity and azimuthal angle `(e, pt, eta, phi)`.\n",
        "\n",
        "The network is considered successful if at least 90% of previously unseen particle four-vectors are attributed a mass with an error of 10%.\n",
        "\n",
        "Input four-vectors are randomly created for each batch following some pre-defined pdfs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsGU1wcJFL3O"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import math\n",
        "import functools\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data generator"
      ],
      "metadata": {
        "id": "S7sNi6Wcn4L7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we define generator of data. It's a simple function that returns a batch of four-vectors in the shape `(n, 4)`. You can control the allowed momentum and mass ranges, as well as the output basis which can be\n",
        "- `cartesion=True` for `(e, px, py, pz)`, or\n",
        "- `cartesion=False` for `(e, pt, eta, phi)`."
      ],
      "metadata": {
        "id": "Ax1C7_a6aUHR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJPv3Pg_FW4E"
      },
      "outputs": [],
      "source": [
        "# data generator\n",
        "def get_data(n, min_p, max_p, min_m, max_m, cartesian=True):\n",
        "    # uniform momentum\n",
        "    p = np.random.uniform(min_p, max_p, n).astype(np.float32)\n",
        "    # uniform masses\n",
        "    m = np.random.uniform(min_m, max_m, n).astype(np.float32)\n",
        "    # isotrop azimuth\n",
        "    phi = np.random.uniform(-math.pi, math.pi, n).astype(np.float32)\n",
        "    # centralized pseudo-rapidity with detector-like coverage\n",
        "    eta = np.zeros_like(p)\n",
        "    while np.mean(abs(eta) > 2.5) > 0.05:\n",
        "        mask = abs(eta) > 2.5\n",
        "        eta[mask] = np.random.normal(0, 2.5, np.sum(mask))\n",
        "    eta = eta.astype(np.float32)\n",
        "    # derive quantities\n",
        "    e = (p * p + m * m)**0.5\n",
        "    theta = 2.0 * np.arctan(np.exp(-eta))\n",
        "    pt = p * np.cos(theta)\n",
        "    if cartesian:\n",
        "        px = pt * np.cos(phi)\n",
        "        py = pt * np.sin(phi)\n",
        "        pz = p * np.sin(theta)\n",
        "        vec = [e, px, py, pz]\n",
        "    else:\n",
        "        vec = [e, pt, eta, phi]\n",
        "    return np.stack(vec, axis=1), m"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neural network (model) definition"
      ],
      "metadata": {
        "id": "xVLAzFKZoBY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we define a function that can generate our model. The implementation below aleady comes with a few bells and whistles, allowing you to configure the overall architecutre, activation functions and different [kernel / weight initializers](https://keras.io/api/layers/initializers/) as well as batch normalization, separate for the inut layer and all hidden layers."
      ],
      "metadata": {
        "id": "SIpPtd86a1Ld"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpAsysbUF3GT"
      },
      "outputs": [],
      "source": [
        "# model definition\n",
        "def get_model(\n",
        "    n_inputs,\n",
        "    n_layers,\n",
        "    n_units,\n",
        "    activation=\"linear\",\n",
        "    kernel_initializer=\"glorot_uniform\",\n",
        "    batch_norm_first=False,\n",
        "    batch_norm=False,\n",
        "):\n",
        "    x = tf.keras.layers.Input(shape=(n_inputs,))\n",
        "    a = x\n",
        "\n",
        "    # standard batch norm for input scaling\n",
        "    if batch_norm_first:\n",
        "        a = tf.keras.layers.BatchNormalization(axis=1)(a)\n",
        "    \n",
        "    # no batch norm when selu is chosen\n",
        "    if activation == \"selu\":\n",
        "        batch_norm = False\n",
        "    \n",
        "    # stack layers\n",
        "    for _ in range(n_layers):\n",
        "        # dense layer\n",
        "        a = tf.keras.layers.Dense(n_units, kernel_initializer=kernel_initializer)(a)\n",
        "        # batch norm before activation, except for relu\n",
        "        if batch_norm and activation != \"relu\":\n",
        "            a = tf.keras.layers.BatchNormalization(axis=1)(a)\n",
        "        # activation\n",
        "        a = tf.keras.layers.Activation(activation)(a)\n",
        "        # batch norm after activation for relu\n",
        "        if batch_norm and activation == \"relu\":\n",
        "            a = tf.keras.layers.BatchNormalization(axis=1)(a)\n",
        "\n",
        "    y = tf.keras.layers.Dense(1, activation=\"linear\", kernel_initializer=\"glorot_normal\")(a)\n",
        "    model = tf.keras.Model(inputs=x, outputs=y)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Setup"
      ],
      "metadata": {
        "id": "atDHyydYoL5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it's time to initialize the model, define the loss (a standard mean squared error loss), and configure the function that will generate our input data batches (`functool.partial` allows to pin-point certain arguments of a function).\n",
        "\n",
        "Also, we can define the function that should scale (and re-scale) the output target value to an eaiser-to-digest (for the network) range."
      ],
      "metadata": {
        "id": "oBw4ndB-bWw7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8O10O_F_JRzy"
      },
      "outputs": [],
      "source": [
        "# create the model, the loss function and the optimizer\n",
        "model = get_model(n_inputs=4, n_layers=2, n_units=8)\n",
        "\n",
        "# loss definition\n",
        "@tf.function\n",
        "def loss_func(pred, truth):\n",
        "    mse = (pred - truth)**2.0\n",
        "    # reduce to average over batch axis\n",
        "    return tf.reduce_mean(mse, axis=0)\n",
        "\n",
        "# define the function that creates a batch with configured values\n",
        "data_spec = {\"min_p\": 0.0, \"max_p\": 200.0, \"min_m\": 0.1, \"max_m\": 100.0, \"cartesian\": True}\n",
        "get_batch = functools.partial(get_data, **data_spec)\n",
        "\n",
        "# function to (re)scale mass to usable (physical) range\n",
        "target_mu = 0.5 * (data_spec[\"min_m\"] + data_spec[\"max_m\"])\n",
        "target_sigma = (data_spec[\"max_m\"] - data_spec[\"min_m\"]) / 12.0**0.5\n",
        "\n",
        "def scale_target(target, rescale=False):\n",
        "    if rescale:\n",
        "        return (target * target_sigma) + target_mu\n",
        "    else:\n",
        "        return (target - target_mu) / (target_sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Single training step"
      ],
      "metadata": {
        "id": "NGLJqid8oQJ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the training to run, we need to define what a single train step does.\n",
        "\n",
        "For that, we define a function accepting the input `x`, the vector of true masses `truth`, and the instance of an `optimizer` that was configured before. With these ingredients, the train step itself is rather simple:\n",
        "\n",
        "1. Perform the forward pass to obtain the network prediction.\n",
        "2. Compare it to the truth in terms of the loss function.\n",
        "3. Guard steps 1. and 2. by a `tf.GradientTape` that tracks gradients in all operations that are performed within its context.\n",
        "4. From that tape, get the gradients of the loss with respect to all trainable variables.\n",
        "5. Tell the optimizer to perform the weight update given the gradients.\n"
      ],
      "metadata": {
        "id": "OADQKoznb-1R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YKzHB_DJ37X"
      },
      "outputs": [],
      "source": [
        "# training step\n",
        "def train_step(x, truth, optimizer):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # get prediction\n",
        "        pred = model(x, training=True)[:, 0]\n",
        "        # get loss\n",
        "        loss = tf.reduce_mean(loss_func(pred, truth))\n",
        "    # get and propagate gradients\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Run the fulll training loop"
      ],
      "metadata": {
        "id": "DRlBHWnaoUk-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All ingredients have be set up and configured at this point, so it's time to define and run our main training loop. It's supposed to do multiple things at once, though in a well defined order:\n",
        "\n",
        "1. Configure and create an optimizer. We are going to use Adam.\n",
        "2. We peform several iterations of\n",
        "  - creating a new batch of input features, and corresponding truth values, and\n",
        "  - perform a training step with the function we defined above.\n",
        "3. After the training iterations, we create a larger batch and run the model prediction for it, so that we can\n",
        "  - compute our success metric, i.e., the fraction of samples with a prediction error of less than 10%, and\n",
        "  - create a few plots to get insights on the model performance."
      ],
      "metadata": {
        "id": "QBo_yEvzmE5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wt6-kr9lM-wi"
      },
      "outputs": [],
      "source": [
        "# main training loop\n",
        "def train_loop(n_iterations, learning_rate=0.003):\n",
        "    # optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "    # run train steps\n",
        "    steps = trange(n_iterations)\n",
        "    for i in steps:\n",
        "        # get a batch of input vectors and true masses\n",
        "        vec, truth = get_batch(n=64)\n",
        "\n",
        "        # do a train step with the scaled mass\n",
        "        loss = train_step(vec, scale_target(truth), optimizer)\n",
        "        steps.set_postfix({\"loss_mse\": round(float(loss), 4)})\n",
        "    \n",
        "    # perform a test with more data\n",
        "    vec, truth = get_batch(n=20000)\n",
        "    pred_raw = model(vec, training=False)[:, 0]\n",
        "    pred = scale_target(pred_raw, rescale=True)\n",
        "    \n",
        "    # relative error\n",
        "    rel_err = (pred - truth) / truth\n",
        "    print(f\"predictions with error < 10%: {100 * np.mean(abs(rel_err) < 0.10):.2f}%\")\n",
        "\n",
        "    # 1d plot or relative error\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(16, 5))\n",
        "    ax1.hist(rel_err, bins=40, range=[-0.5, 0.5])\n",
        "    ax1.set_xlim(-0.5, 0.5)\n",
        "    ax1.set_xlabel(\"Relative error\")\n",
        "    ax1.set_ylabel(\"Entries\")\n",
        "\n",
        "    # 2d plot vs. E\n",
        "    ax2.hist2d(rel_err, vec[:, 0], bins=[20, 20], range=[[-0.5, 0.5], [0, 200.0]])\n",
        "    ax2.set_xlim(-0.5, 0.5)\n",
        "    ax2.set_xlabel(\"Relative error\")\n",
        "    ax2.set_ylabel(\"E\")\n",
        "\n",
        "    # 2d plot vs. gamma\n",
        "    gamma = vec[:, 0] / truth\n",
        "    ax3.hist2d(rel_err, gamma, bins=[20, 20], range=[[-0.5, 0.5], [1, 7]])\n",
        "    ax3.set_xlim(-0.5, 0.5)\n",
        "    ax3.set_xlabel(\"Relative error\")\n",
        "    ax3.set_ylabel(r\"$\\gamma$ (E / m)\")\n",
        "\n",
        "    return pred, truth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiT25peHUWXl"
      },
      "outputs": [],
      "source": [
        "# a couple quick runs with large learning rates\n",
        "pred, truth = train_loop(300, 0.03)\n",
        "pred, truth = train_loop(300, 0.003)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# longer training with a smaller rate\n",
        "pred, truth = train_loop(2000, 0.001)"
      ],
      "metadata": {
        "id": "P96-4GX706SZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "How did your network perform?\n",
        "Try to improve it 👾"
      ],
      "metadata": {
        "id": "V8UJIdhjoZfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "... ❗️"
      ],
      "metadata": {
        "id": "jZthrd06oiJx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "mass_regression.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}